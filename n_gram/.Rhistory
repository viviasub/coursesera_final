library(tm)
library(RWekajars)
library(RWeka)
library(wordcloud)
require(openNLP)
require(reshape)
set.seed(892)
sample_pct <- .005
#Get files locations
us_txt_dir <- "en_US/"
blogs_txt <- paste(us_txt_dir, "en_US.blogs.txt", sep = "")
news_txt <- paste(us_txt_dir, "en_US.news.txt", sep = "")
twitter_txt <- paste(us_txt_dir, "en_US.twitter.txt", sep = "")
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines(blogs_txt, skipNul = T))
news_txt <- paste(us_txt_dir, "en_US.news.txt", sep = "")
twitter_txt <- paste(us_txt_dir, "en_US.twitter.txt", sep = "")
blogs_txt <- paste(us_txt_dir, "en_US.blogs.txt", sep = "")
news_txt <- paste(us_txt_dir, "en_US.news.txt", sep = "")
twitter_txt <- paste(us_txt_dir, "en_US.twitter.txt", sep = "")
news_data <- tolower(readLines(news_txt, skipNul = T))
twitter_data <- tolower(readLines(twitter_txt, skipNul = T))
######################## Analyze News Data
news_data_sample <- news_data[sample(1:news_lines, news_lines*sample_pct)]
news_lines<-length(news_data)
######################## Analyze News Data
news_data_sample <- news_data[sample(1:news_lines, news_lines*sample_pct)]
news_cp <- Corpus(VectorSource(list(news_data_sample)))
#Clean up corpus
news_cp <- tm_map(news_cp, removeNumbers)
news_cp <- tm_map(news_cp, removePunctuation)
news_cp <- tm_map(news_cp, removeWords, stopwords('english'))
news_cp <- tm_map(news_cp, stripWhitespace)
#Create doc term matrix
news_dtm <- DocumentTermMatrix(news_cp)
#Find frequent words
news_dtm_mtrx <- as.matrix(news_dtm)
news_frequency <- colSums(news_dtm_mtrx)
news_frequency <- sort(news_frequency, decreasing = TRUE)
saveRDS(news_frequency, "data/news_frequency.Rda")
######################## Analyze Twitter Data
twitter_lines<-length(twitter_data)
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*sample_pct)]
gc()
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*sample_pct)]
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*0.001)]
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*0.0001)]
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*sample_pct)]
rm(news_data)
gc()
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*sample_pct)]
gc()
library(tm)
library(RWekajars)
library(RWeka)
library(wordcloud)
require(reshape)
set.seed(892)
sample_pct <- .005
#Get files locations
us_txt_dir <- "en_US/"
blogs_txt <- paste(us_txt_dir, "en_US.blogs.txt", sep = "")
news_txt <- paste(us_txt_dir, "en_US.news.txt", sep = "")
twitter_txt <- paste(us_txt_dir, "en_US.twitter.txt", sep = "")
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines(blogs_txt, skipNul = T))
news_data <- tolower(readLines(news_txt, skipNul = T))
twitter_data <- tolower(readLines(twitter_txt, skipNul = T))
#bad_words_data <- readLines(bad_words_txt, skipNul = T)
rm(list=ls(pattern="txt"))
######################## Analyze News Data
news_data_sample <- news_data[sample(1:news_lines, news_lines*sample_pct)]
news_lines<-length(news_data)
######################## Analyze News Data
news_data_sample <- news_data[sample(1:news_lines, news_lines*sample_pct)]
news_cp <- Corpus(VectorSource(list(news_data_sample)))
#Clean up corpus
news_cp <- tm_map(news_cp, removeNumbers)
news_cp <- tm_map(news_cp, removePunctuation)
news_cp <- tm_map(news_cp, removeWords, stopwords('english'))
news_cp <- tm_map(news_cp, stripWhitespace)
#Create doc term matrix
news_dtm <- DocumentTermMatrix(news_cp)
rm(news_data)
#Find frequent words
news_dtm_mtrx <- as.matrix(news_dtm)
news_frequency <- colSums(news_dtm_mtrx)
news_frequency <- sort(news_frequency, decreasing = TRUE)
saveRDS(news_frequency, "data/news_frequency.Rda")
######################## Analyze Twitter Data
twitter_lines<-length(twitter_data)
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*sample_pct)]
######################## Analyze Twitter Data
twitter_lines<-length(twitter_data)/2
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*sample_pct)]
######################## Analyze Twitter Data
twitter_lines<-length(twitter_data)/4
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*sample_pct)]
twitter_cp <- Corpus(VectorSource(list(twitter_data_sample)))
#Clean up corpus
twitter_cp <- tm_map(twitter_cp, removeNumbers)
twitter_cp <- tm_map(twitter_cp, removePunctuation)
twitter_cp <- tm_map(twitter_cp, removeWords, stopwords('english'))
twitter_cp <- tm_map(twitter_cp, stripWhitespace)
#Create doc term matrix
twitter_dtm <- DocumentTermMatrix(twitter_cp)
#Find frequent words
twitter_dtm_mtrx <- as.matrix(twitter_dtm)
twitter_frequency <- colSums(twitter_dtm_mtrx)
twitter_frequency <- sort(twitter_frequency, decreasing = TRUE)
saveRDS(twitter_frequency, "data/twitter_frequency.Rda")
library(tm)
library(RWekajars)
library(RWeka)
library(wordcloud)
require(openNLP)
require(reshape)
set.seed(892)
#Get files locations
us_txt_dir <- "en_US/"
blogs_txt <- paste(us_txt_dir, "en_US.blogs.txt", sep = "")
news_txt <- paste(us_txt_dir, "en_US.news.txt", sep = "")
twitter_txt <- paste(us_txt_dir, "en_US.twitter.txt", sep = "")
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines(blogs_txt, skipNul = T))
news_data <- tolower(readLines(news_txt, skipNul = T))
twitter_data <- tolower(readLines(twitter_txt, skipNul = T))
#bad_words_data <- readLines(bad_words_txt, skipNul = T)
rm(list=ls(pattern="txt"))
#Get files locations
us_txt_dir <- "en_US/"
blogs_txt <- paste(us_txt_dir, "en_US.blogs.txt", sep = "")
news_txt <- paste(us_txt_dir, "en_US.news.txt", sep = "")
twitter_txt <- paste(us_txt_dir, "en_US.twitter.txt", sep = "")
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines(blogs_txt, skipNul = T))
#Source http://www.freewebheaders.com/full-list-of-bad-words-banned-by-google/
#bad_words_txt <- paste(us_txt_dir, "bad-words-by-google.txt", sep = "")
gc()
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines(blogs_txt, skipNul = T))
library(tm)
#library(RWekajars)
library(RWeka)
library(wordcloud)
#require(openNLP)
require(reshape)
#Get files locations
us_txt_dir <- "en_US/"
blogs_txt <- paste(us_txt_dir, "en_US.blogs.txt", sep = "")
news_txt <- paste(us_txt_dir, "en_US.news.txt", sep = "")
twitter_txt <- paste(us_txt_dir, "en_US.twitter.txt", sep = "")
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines(blogs_txt, skipNul = T))
news_data <- tolower(readLines(news_txt, skipNul = T))
twitter_data <- tolower(readLines(twitter_txt, skipNul = T))
#bad_words_data <- readLines(bad_words_txt, skipNul = T)
rm(list=ls(pattern="txt"))
news_lines<-length(news_data)
blogs_lines<-length(blogs_data)
twitter_lines<-length(twitter_data)
######################## Analyze Full Data
#Create smaller samples for further processing of combined corpus
sample_pct <- .01
blogs_data_sample <- blogs_data[sample(1:blogs_lines, blogs_lines*sample_pct)]
news_data_sample <- news_data[sample(1:news_lines, news_lines*sample_pct)]
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*sample_pct)]
sample_data <- list(blogs_data_sample, news_data_sample, twitter_data_sample)
rm(list=ls(pattern="news"))
rm(list=ls(pattern="twitter"))
rm(list=ls(pattern="blogs"))
gc()
#Create Corpus
cp <- Corpus(VectorSource(sample_data))
#Clean up corpus
#cp <- tm_map(cp, removeWords, bad_words_data )
cp <- tm_map(cp, removeNumbers)
cp <- tm_map(cp, removePunctuation)
cp <- tm_map(cp, stripWhitespace)
#Create doc term matrix
dtm <- DocumentTermMatrix(cp, control = list(stopwords = TRUE))
require(openNLP)
#Create doc term matrix
dtm <- DocumentTermMatrix(cp, control = list(stopwords = TRUE))
library(RWekajars)
#Create doc term matrix
dtm <- DocumentTermMatrix(cp, control = list(stopwords = TRUE))
#Create doc term matrix
dtm <- DocumentTermMatrix(cp)
?DocumentTermMatrix
#Create Corpus
cp <- Corpus(VectorSource(sample_data))
#Create doc term matrix
dtm <- DocumentTermMatrix(cp)
gc()
#Create doc term matrix
dtm <- DocumentTermMatrix(cp)
#Create doc term matrix
dtm <- DocumentTermMatrix(cp, control = list(stopwords = TRUE))
#Clean up corpus
#cp <- tm_map(cp, removeWords, bad_words_data )
cp <- tm_map(cp, removeNumbers)
library(tm)
#library(wordcloud)
#require(openNLP)
require(reshape)
#Get files locations
us_txt_dir <- "en_US/"
blogs_txt <- paste(us_txt_dir, "en_US.blogs.txt", sep = "")
news_txt <- paste(us_txt_dir, "en_US.news.txt", sep = "")
twitter_txt <- paste(us_txt_dir, "en_US.twitter.txt", sep = "")
#Load text into R. Lowecase to normalize future operations
blogs_data <- tolower(readLines(blogs_txt, skipNul = T))
news_data <- tolower(readLines(news_txt, skipNul = T))
twitter_data <- tolower(readLines(twitter_txt, skipNul = T))
#bad_words_data <- readLines(bad_words_txt, skipNul = T)
rm(list=ls(pattern="txt"))
news_lines<-length(news_data)
blogs_lines<-length(blogs_data)
twitter_lines<-length(twitter_data)
######################## Analyze Full Data
#Create smaller samples for further processing of combined corpus
sample_pct <- .01
blogs_data_sample <- blogs_data[sample(1:blogs_lines, blogs_lines*sample_pct)]
news_data_sample <- news_data[sample(1:news_lines, news_lines*sample_pct)]
twitter_data_sample <- twitter_data[sample(1:twitter_lines, twitter_lines*sample_pct)]
sample_data <- list(blogs_data_sample, news_data_sample, twitter_data_sample)
rm(list=ls(pattern="news"))
rm(list=ls(pattern="twitter"))
rm(list=ls(pattern="blogs"))
gc()
#Create Corpus
cp <- Corpus(VectorSource(sample_data))
#Clean up corpus
#cp <- tm_map(cp, removeWords, bad_words_data )
cp <- tm_map(cp, removeNumbers)
cp <- tm_map(cp, removePunctuation)
cp <- tm_map(cp, stripWhitespace)
ngramTokenizer <- function(l) {
function(x) unlist(lapply(ngrams(words(x), l), paste, collapse = " "), use.names = FALSE)
}
#generate unigram data set
generateNgramData <- function(n) {
if(n == 1) {
ng_tdm <- TermDocumentMatrix(cp)
} else {
ng_tdm <- TermDocumentMatrix(cp, control = list(tokenize = ngramTokenizer(n)))
}
ng_matrix <- as.matrix(ng_tdm)
ng_matrix <- rowSums(ng_matrix)
ng_matrix <- sort(ng_matrix, decreasing = TRUE)
final_ngram <- data.frame(terms = names(ng_matrix), freq = ng_matrix)
if(n == 2) columns <- c('one', 'two')
if(n == 3) columns <- c('one', 'two', 'three')
if(n == 4) columns <- c('one', 'two', 'three', 'four')
if(n > 1) {
final_ngram <- transform(final_ngram, terms = colsplit(terms, split = " ", names = columns ))
}
rownames(final_ngram) <- NULL
final_ngram
}
final_unigram <- generateNgramData(1)
final_bigram <- generateNgramData(2)
final_unigram
final_unigram[1]
final_unigram[[1]]
final_unigram$terms[1]
final_bigram <- generateNgramData(2)
final_unigram <- transform(final_unigram, p = freq / unigram_count, pw = 0)
#Calculate probabilities
unigram_count <- sum(final_unigram$freq)
final_unigram <- transform(final_unigram, p = freq / unigram_count, pw = 0)
class(final_unigram)
head(final_unigram,10)
?trasform
?transform
final_trigram <- generateNgramData(3)
ng_tdm <- TermDocumentMatrix(cp, control = list(tokenize = ngramTokenizer(2)))
ng_matrix <- as.matrix(ng_tdm)
ng_matrix
ng_matrix <- as.matrix(ng_tdm)
ng_matrix <- rowSums(ng_matrix)
ng_matrix <- sort(ng_matrix, decreasing = TRUE)
final_ngram <- data.frame(terms = names(ng_matrix), freq = ng_matrix)
head(final_ngram,10)
columns <- c('one', 'two')
head(final_ngram,10)
#Create Corpus
cp <- VCorpus(VectorSource(sample_data))
#Clean up corpus
#cp <- tm_map(cp, removeWords, bad_words_data )
cp <- tm_map(cp, removeNumbers)
cp <- tm_map(cp, removePunctuation)
cp <- tm_map(cp, stripWhitespace)
ngramTokenizer <- function(l) {
function(x) unlist(lapply(ngrams(words(x), l), paste, collapse = " "), use.names = FALSE)
}
#generate unigram data set
generateNgramData <- function(n) {
if(n == 1) {
ng_tdm <- TermDocumentMatrix(cp)
} else {
ng_tdm <- TermDocumentMatrix(cp, control = list(tokenize = ngramTokenizer(2)))
}
ng_matrix <- as.matrix(ng_tdm)
ng_matrix <- rowSums(ng_matrix)
ng_matrix <- sort(ng_matrix, decreasing = TRUE)
final_ngram <- data.frame(terms = names(ng_matrix), freq = ng_matrix)
if(n == 2) columns <- c('one', 'two')
if(n == 3) columns <- c('one', 'two', 'three')
if(n == 4) columns <- c('one', 'two', 'three', 'four')
if(n > 1) {
final_ngram <- transform(final_ngram, terms = colsplit(terms, split = " ", names = columns ))
}
rownames(final_ngram) <- NULL
final_ngram
}
final_unigram <- generateNgramData(1)
final_bigram <- generateNgramData(2)
head(final_bigram,10)
ng_tdm <- TermDocumentMatrix(cp, control = list(tokenize = ngramTokenizer(2)))
ng_matrix <- as.matrix(ng_tdm)
ng_matrix <- rowSums(ng_matrix)
ng_matrix <- sort(ng_matrix, decreasing = TRUE)
final_ngram <- data.frame(terms = names(ng_matrix), freq = ng_matrix)
final_trigram <- generateNgramData(3)
final_trigram <- generateNgramData(3)
final_bigram <- generateNgramData(2)
bigram_count <- sum(final_bigram$freq)
final_bigram <- transform(final_bigram, p = freq / bigram_count, pone = 0, termone = terms$one, termtwo = terms$two, terms = NULL)
head(final_bigram,10)
rm(list = ls())
closeAllConnections()
dev.set(2)
plot.new()
try(dev.off())
shell("cls")
gc()
#dir<-"Progetti_R/casa"
dir<-"script_vecchi_coursera"
## SET working dir
work_dir<-paste("~/",dir,"/final",sep="")
setwd(work_dir)
setwd("./n_grams")
##############
#Read Files
##############
file1<-"Unigram2_"
DF<-"DF"
no<-"nostop_"
load(paste(file1,no,DF,sep=""))
getwd()
setwd("./n_grams")
setwd(work_dir)
setwd("./n_grams")
setwd("./n_gram")
load(paste(file1,no,DF,sep=""))
load(paste(file1,no,DF,".RData",sep=""))
load(paste(file2,no,DF,".RData",sep=""))
file2<-"Bigram_"
file3<-"Trigram_"
file4<-"Quadrigram_"
DF<-"DF"
no<-"nostop_"
load(paste(file2,no,DF,".RData",sep=""))
head(Bigram_nostop_DF,10)
file2<-"Bigram_nostop_DF.RData"
?trasform
?transform
tmp <- transform(file2, ngrams = colsplit(ngrams, split = " ", names = columns ))
class(Bigram_nostop_DF)
require(reshape)
library("reshape")
tmp <- transform(file2, ngrams = colsplit(ngrams, split = " ", names = columns ))
tmp <- transform(file2, ngrams = colsplit(ngrams, split = " ", names = columns ))
?colsplit
tmp <- transform(file2, colsplit(ngrams, split = " ", names = columns ))
tmp <- transform(Bigram_nostop_DF, colsplit(ngrams, split = " ", names = columns ))
columns<-c("one","two")
tmp <- transform(Bigram_nostop_DF, colsplit(ngrams, split = " ", names = columns ))
head(tmp,10)
tmp <- transform(Bigram_nostop_DF, ngrams=colsplit(ngrams, split = " ", names = columns ))
head(tmp,10)
head(tmp,20)
load(file2)
Bigram_nostop_DF <- transform(file2,
ngrams=colsplit(ngrams, split = " ",
names = columns ))
rm(list = ls())
closeAllConnections()
try(dev.off())
dev.set(2)
plot.new()
shell("cls")
gc()
library("tm")
library("reshape")
#dir<-"Progetti_R/casa"
dir<-"script_vecchi_coursera"
## SET working dir
work_dir<-paste("~/",dir,"/final",sep="")
setwd(work_dir)
setwd("./n_gram")
##############
#Read Files
##############
#file1<-"Unigram2_nostop_DF.RData"
file2<-"Bigram_nostop_DF.RData"
file3<-"Trigram_nostop_DF.RData"
file4<-"Quadrigram_nostop_DF.Rdata"
columns<-c("one","two")
tmp<-load(file2)
Bigram_nostop_DF <- transform(load(file2),
ngrams=colsplit(ngrams, split = " ",
names = columns ))
load(file2)
Bigram_nostop_DF <- transform(Bigram_nostop_DF,
ngrams=colsplit(ngrams, split = " ",
names = columns ))
load(file3)
Trigram_nostop_DF <- transform(Trigram_nostop_DF,
ngrams=colsplit(ngrams, split = " ",
names = columns ))
load(file4)
Quadrigram_nostop_DF <- transform(Quadrigram_nostop_DF,
ngrams=colsplit(ngrams, split = " ",
names = columns ))
head(Bigram_nostop_DF,20)
head(Trigram_nostop_DF,20)
columns<-c("one","two","three","four")
load(file2)
Bigram_nostop_DF <- transform(Bigram_nostop_DF,
ngrams=colsplit(ngrams, split = " ",
names = columns ))
load(file2)
columns<-c("one","two")
Bigram_nostop_DF <- transform(Bigram_nostop_DF,
ngrams=colsplit(ngrams, split = " ",
names = columns ))
load(file3)
columns<-c("one","two","three")
Trigram_nostop_DF <- transform(Trigram_nostop_DF,
ngrams=colsplit(ngrams, split = " ",
names = columns ))
load(file4)
columns<-c("one","two","three","four")
Quadrigram_nostop_DF <- transform(Quadrigram_nostop_DF,
ngrams=colsplit(ngrams, split = " ",
names = columns ))
getwd()
save(Quadrigram_DF,file="../split_n_gram/Bigram_nostop_DF.RData")
save(Bigram_nostop_D,file="../split_n_gram/Bigram_nostop_DF.RData")
save(Bigram_nostop_DF,file="../split_n_gram/Bigram_nostop_DF.RData")
save(Bigram_nostop_DF,file="../split_ngram/Bigram_nostop_DF.RData")
save(Trigram_nostop_DF,file="../split_ngram/Trigram_nostop_DF.RData")
save(Quadrigram_nostop_DF,file="../split_ngram/Quadrigram_nostop_DF.RData")
#file1<-"Unigram2_nostop_DF.RData"
file2<-"Bigram_nostop_DF.RData"
file3<-"Trigram_nostop_DF.RData"
file4<-"Quadrigram_nostop_DF.Rdata"
columns<-c("one","two","three","four")
#file1<-"Unigram2_DF.RData"
file2<-"Bigram_DF.RData"
file3<-"Trigram_DF.RData"
file4<-"Quadrigram_DF.Rdata"
load(file2)
columns<-c("one","two")
Bigram_DF <- transform(Bigram_DF,
ngrams=colsplit(ngrams, split = " ",
names = columns ))
head(Bigram_DF,10)
save(Bigram_DF,file="../split_ngram/Bigram_DF.RData")
load(file3)
columns<-c("one","two","three")
Trigram_DF <- transform(Trigram_DF,
ngrams=colsplit(ngrams, split = " ",
names = columns ))
head(igram_DF,10)
head(Trigram_DF,10)
save(Trigram_DF,file="../split_ngram/Trigram_DF.RData")
load(file4)
columns<-c("one","two","three","four")
Quadrigram_DF <- transform(Quadrigram_DF,
ngrams=colsplit(ngrams, split = " ",
names = columns ))
rm(list=ls(pattern = "Big"))
rm(list=ls(pattern = "Trig"))
rm(list=ls(pattern = "Quad"))
gc()
load(file4)
columns<-c("one","two","three","four")
Quadrigram_DF <- transform(Quadrigram_DF,
ngrams=colsplit(ngrams, split = " ",
names = columns ))
head(Quadrigram_DF,10)
save(Quadrigram_DF,file="../split_ngram/Quadrigram_DF.RData")
