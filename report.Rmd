---
title: "Data Science Specialization - Milestone Report"
author: "Antonio Vivi"
date: '`r paste("First created on Feb 24, 2019. Updated on", Sys.Date())`'
output: html_document
---


This is the milestone Report of the  final capstone project for the Coursera Data Science Specialization and explains the exploratory analysis and goals for the eventual app and algorithm to create an application that predicts the next word in a phrase or sentence. We will use natural language processing techniques, [N-grams/Markov Models](https://en.wikipedia.org/wiki/n-grams) to evaluate statistical probabilities on the appearance of prior word(s). The data used are a variety of text documents.

## Available Data
I get the zip file of corpus files [downloadable here](http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). These files contained a set of twitter, news and blog text in 4 different languages. I will focus on the provided US English set of documents.

```{r setoptions, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(warning = FALSE, message = FALSE)
dir<-"Progetti_R/casa"
#dir<-"script_vecchi_coursera"

work_dir<-paste("~/",dir,"/final",sep="")
setwd(work_dir)
load("textstat.rda")
load("data_clean/twitter.rda")
load("data_clean/news.rda")
load("data_clean/blogs.rda")
```

## Data Analysis
Below, is a summary of the 3 corpus documents.

```{r echo=FALSE}
kable(textstat, 
      format.args = list(big.mark = ','),
      col.names = c("File","File Size (MB)", "Num. of Lines", "Longest Line", "Num. of Words"))
```

Blogs, is the largest file, has heigher number of words but the lower number of lines. The longest line in the blogs file indicate that this file comprises of longer sentences.  News, is the second larger file and has the second largest number of lines. The news file looks has good middle ground for sentence length and variety of word usage and ideas. Twitter file has the short phrases (maximum 140 characters). The Twitter file consequently has the most number of lines (over 2 million) and the least number of words (over 30 million).

### Sampling
Due to the large size of these files, processing the entire contents on a local computer proved to be very time consuming. So, I chose to sample the data. For each individual corpus, I chose a variable randomly sample of 5% for blogs , 40% for news and 5% twitter, than I combined the files into a single "file of the"corpus" document prior to performing exploratory analysis. 

```{r echo=TRUE, eval=FALSE}

twitter<-sample(twitter,twitter_lines*0.05, replace=F)
blogs <- sample(blogs, blogs_lines*0.05,replace = F)
news <- sample(news, news_lines*0.4,replace = F)

```

### Common Words
Words were analyzed in the corpus files. The  word cloud and barplotgraphics below show the details.

## Plot Figures

### Unigram
```{r, echo=FALSE,out.width="49%", out.height="30%"}
include_graphics(c("fig/Unigram.png","fig/Unigram_bar.png"))
```

### Bigram
```{r, echo=FALSE,out.width="49%", out.height="30%"}
include_graphics(c("fig/Bigram.png","fig/Bigram_bar.png"))
```

### Trigram
```{r, echo=FALSE,out.width="49%", out.height="30%"}
include_graphics(c("fig/Trigram.png","fig/Trigram_nostop_bar.png"))
```

### Unigram without nostop word
```{r, echo=FALSE,out.width="49%", out.height="30%"}
include_graphics(c("fig/Unigram_nostop.png","fig/Unigram_nostop_bar.png"))
```

### Bigram without nostop word
```{r, echo=FALSE,out.width="49%", out.height="30%"}
include_graphics(c("fig/Bigram_nostop.png","fig/Bigram_nostop_bar.png"))
```

### Trigram without nostop word
```{r, echo=FALSE,out.width="49%", out.height="30%"}
include_graphics(c("fig/Trigram_nostop.png","fig/Trigram_nostop_bar.png"))
```

## Future Plans
I use the sampled data analyzed above to create a statistical model (based on [N-grams/Markov Models](https://en.wikipedia.org/wiki/n-grams)) to predict the next word in a phrase/sentence. The model created will subsequently be tested.

\newpage
## Appendix

Below is the full R processing code used to generate this report:

### Read and Clean Data

```{r eval=FALSE}
rm(list = ls())
try(dev.off())
dev.set(2)
plot.new()
shell("cls")

library("stringr")
library("stringi")
library("tm")

dir<-"Progetti_R/casa"
#dir<-"script_vecchi_coursera"

## SET working dir 
work_dir<-paste("~/",dir,"/final",sep="")
setwd(work_dir)
setwd("./en_US")

##############
#Read Files
##############

con <- file("en_US.twitter.txt","r")
twitter<-readLines(con,encoding="UTF-8",skipNul=TRUE)
## Read the next line of text readLines(con, 5) 
close(con) 
## It's important to close the connection when you are done

con <- file("en_US.blogs.txt", "r")
blogs<-readLines(con,encoding="UTF-8",skipNul=TRUE)
close(con) 

con <- file("en_US.news.txt", "r")
news<-readLines(con,encoding="UTF-8",skipNul=TRUE)
close(con)

setwd("..")

rm(con,dir)


#get file size
twitter_size<-object.size(twitter)
twitter_size<-capture.output(print(twitter_size,units="Mb"))

blogs_size<-object.size(blogs)
blogs_size<-capture.output(print(blogs_size,units="Mb"))

news_size<-object.size(news)
news_size<-capture.output(print(news_size,units="Mb"))

#get line count
twitter_lines<-length(twitter)
blogs_lines<-length(blogs)
news_lines<-length(news)

#get max char line count
twitter_str_lenght<-str_length(twitter)
twitter_max<-max(twitter_str_lenght)
blogs_str_lenght<-str_length(blogs)
blogs_max<-max(blogs_str_lenght)
news_str_lenght<-str_length(news)
news_max<-max(news_str_lenght)

#(index<-which.max(twitter_str_lenght))
#str_length(twitter[index])
#twitter[index]

#get count words
twitter_count<-sum(stri_count_words(twitter))
blogs_count<-sum(stri_count_words(blogs))
news_count<-sum(stri_count_words(news))



textstat<-data.frame ("files"=c("twitter","blogs","news"),
                      "Size"=c(twitter_size,blogs_size,news_size),
                      "Number of Lines"=c(twitter_lines,blogs_lines,news_lines),
                      "Longest Line"=c(twitter_max,blogs_max,news_max),
                      "Numbers of Word"=c(twitter_count,blogs_count,news_count)
)

save(textstat,file="textstat.rda")

#rm(list=ls(pattern="_"))

###############
# sample data
###############

twitter<-sample(twitter,twitter_lines*0.05, replace=F)
blogs <- sample(blogs, blogs_lines*0.05,replace = F)
news <- sample(news, news_lines*0.4,replace = F)


##############
# Clean data
##############

twitter<-tolower(twitter)
blogs<-tolower(blogs)
news<-tolower(news)

#pattern1<-"http"
#tmp1<-grep(pattern1,twitter)
#twitter[tmp1[1:10]]

#remove URL
pattern_html<-"http([^ ]+)"
pattern_html_1<-"http://[[:space:]]"
twitter<-gsub(pattern_html_1, "http://", twitter)
blogs<-gsub(pattern_html_1,"http://",blogs)
news<-gsub(pattern_html_1,"http://",news)
twitter<-gsub(pattern_html, "", twitter)
blogs<-gsub(pattern_html,"",blogs)
news<-gsub(pattern_html,"",news)

#twitter<-gsub(pattern1,"",twitter)

#remove all not english letter and space
twitter<-gsub("[^[:alpha:][:space:]]*", "", twitter)
blogs<-gsub("[^[:alpha:][:space:]]*", "", blogs)
news<-gsub("[^[:alpha:][:space:]]*", "", news)

save(twitter,file="data_clean/twitter.rda")
save(blogs,file="data_clean/blogs.rda")
save(news,file="data_clean/news.rda")

rm(list=ls())
```

### Create a Corpus

```{r eval=FALSE}
rm(list = ls())
try(dev.off())
dev.set(2)
plot.new()
shell("cls")
gc()

library("tm")

dir<-"Progetti_R/casa"
#dir<-"script_vecchi_coursera"

## SET working dir 
work_dir<-paste("~/",dir,"/final",sep="")
setwd(work_dir)
setwd("./data_clean")

##############
#Read Files
##############

load("twitter.rda")
load("blogs.rda")
load("news.rda")

#setwd("../data_R")

rm(dir)

#################
# Sampling data
#################

data_sample<-c(twitter,blogs,news)
save(data_sample,file="data_sample.rda")

rm(list=ls(pattern="blogs"))
rm(list=ls(pattern="news"))
rm(list=ls(pattern="twitter"))
gc()
 
#corpus<-SimpleCorpus(VectorSource(list(data_sample))) #OK
corpus<-Corpus(VectorSource(data_sample))#very big file
rm("data_sample")

#Remove profanity and other words we do not want to predict. 
# file available in this locations:
#(https://github.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en)
profanity <- readLines("../en_US/profanity.txt")

#################
# Data cleaning
#################

#corpus <- tm_map(corpus, removeNumbers)
#corpus <- tm_map(corpus, removePunctuation)
#corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, profanity)
save(corpus,file="corpus.RData")

corpus_nostop <- tm_map(corpus, removeWords, stopwords('english'))

save(corpus_nostop,file="corpus_nostop.RData")
```

### n-gams genetation

```{r eval=FALSE}
rm(list = ls())
closeAllConnections()
try(dev.off())
dev.set(2)
plot.new()
shell("cls")
gc()

library("tm")
library("ngram")

dir<-"Progetti_R/casa"
#dir<-"script_vecchi_coursera"

## SET working dir 
work_dir<-paste("~/",dir,"/final",sep="")
setwd(work_dir)
setwd("./data_clean")

##############
#Read Files
##############
file1<-"corpus.RData"
file2<-"corpus_nostop.RData"

rm(dir)

########################
## Tokenizeing N-grams
########################

load(file1)
Unigram <- DocumentTermMatrix(corpus)
Unigram <- removeSparseTerms(Unigram, 0.98)
rm(corpus)
gc()
sort_Unigram <- sort(colSums(as.matrix(Unigram)),decreasing=TRUE)
Unigram_DF <- data.frame(word = names(sort_Unigram),freq=sort_Unigram)
head(Unigram_DF, 20)
save(Unigram_DF,file="../n_gram/Unigram_DF.RData")
rm(list = ls(pattern = "Unigram"))
gc()

load(file1)
str <- concatenate ( lapply ( corpus , "[", 1) )
rm(corpus)

Bigram <- ngram (str , n =2)
Bigram_DF<-get.phrasetable (Bigram) #generate data.frame
head(Bigram_DF,20)
save(Bigram_DF,file="../n_gram/Bigram_DF.RData")
rm(list=ls(pattern="Bigram"))

Trigram <- ngram (str , n =3)
Trigram_DF<-get.phrasetable (Trigram) #generate data.frame
head(Trigram_DF,20)
save(Trigram_DF,file="../n_gram/Trigram_DF.RData")
rm(list=ls(pattern="Trigram"))

Quadrigram <- ngram (str , n =4)
Quadrigram_DF<-get.phrasetable (Quadrigram) #generate data.frame
head(Quadrigram_DF,20)
save(Quadrigram_DF,file="../n_gram/Quadrigram_DF.RData")
rm(list=ls(pattern="Trigram"))

########################
## Tokenizeing N-grams
# no_stop_word
########################


load(file2)

Unigram <- DocumentTermMatrix(corpus_nostop)
Unigram <- removeSparseTerms(Unigram, 0.98)
rm(corpus_nostop)
gc()
sort_Unigram <- sort(colSums(as.matrix(Unigram)),decreasing=TRUE)
Unigram_nostop_DF <- data.frame(word = names(sort_Unigram),freq=sort_Unigram)
head(Unigram_nostop_DF, 20)
save(Unigram_nostop_DF,file="../n_gram/Unigram_nostop_DF.RData")
rm(list = ls(pattern = "Unigram"))
gc()

load(file2)
str <- concatenate ( lapply ( corpus_nostop , "[", 1) )
rm(corpus_nostop)

Bigram <- ngram (str , n =2)
Bigram_nostop_DF<-get.phrasetable (Bigram) #generate data.frame
head(Bigram_nostop_DF,20)
save(Bigram_nostop_DF,file="../n_gram/Bigram_nostop_DF.RData")
rm(list=ls(pattern="Bigram"))

Trigram <- ngram (str , n =3)
Trigram_nostop_DF<-get.phrasetable (Trigram) #generate data.frame
head(Trigram_nostop_DF,20)
save(Trigram_nostop_DF,file="../n_gram/Trigram_nostop_DF.RData")
rm(list=ls(pattern="Trigram"))

Quadrigram <- ngram (str , n =4)
Quadrigram_nostop_DF<-get.phrasetable (Quadrigram) #generate data.frame
head(Quadrigram_nostop_DF,20)
save(Quadrigram_nostop_DF,file="../n_gram/Quadrigram_nostop_DF.RData")
rm(list=ls(pattern="Quadrigram"))
```

### Plot 

```{r eval=FALSE}
rm(list = ls())
closeAllConnections()
try(dev.off())
dev.set(2)
plot.new()
shell("cls")
gc()

library("tm")
library("wordcloud")

dir<-"Progetti_R/casa"
#dir<-"script_vecchi_coursera"

## SET working dir 
work_dir<-paste("~/",dir,"/final",sep="")
setwd(work_dir)
setwd("./n_gram")

##############
#Read Files
##############

load("Unigram_DF.RData")
load("Unigram_nostop_DF.RData")

rm(dir)

head(Unigram_DF, 20)

############
# plot
############
setwd("../fig")
png("Unigram.png", width=480, height=480)
#wordcloud(Unigram_DF$word, Unigram_DF$freq,scale = c(3,3) ,colors = brewer.pal(8, "Spectral"))
wordcloud(words = Unigram_DF$word, freq = Unigram_DF$freq, min.freq = 100,
         max.words=80, random.order=FALSE, rot.per=0.25, 
         colors=brewer.pal(8, "Dark2"),scale=c(5,1.5))
dev.off()

png("Unigram_bar.png", width=480, height=480)
barplot(Unigram_DF[1:10,]$freq, las = 2, 
        names.arg = Unigram_DF[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
dev.off()

png("Unigram_nostop.png", width=480, height=480)
wordcloud(words = Unigram_nostop_DF$word, freq = Unigram_nostop_DF$freq, min.freq =100,
          max.words=100, random.order=FALSE, rot.per=0.25, 
          colors=brewer.pal(8, "Dark2"),scale=c(3,1))
dev.off()

png("Unigram_nostop_bar.png", width=480, height=480)
barplot(Unigram_nostop_DF[1:10,]$freq, las = 2, 
        names.arg = Unigram_nostop_DF[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
dev.off()

setwd("../n_gram")
load("Bigram_DF.RData")
load("Bigram_nostop_DF.RData") #### rigenerare

setwd("../fig")
png("Bigram.png", width=480, height=480)
wordcloud(words = Bigram_DF$ngrams, freq = Bigram_DF$freq, min.freq = 10,
          max.words=50, random.order=FALSE, rot.per=0.25, 
          colors=brewer.pal(8, "Dark2"),scale=c(3,1))
dev.off()

png("Bigram_bar.png", width=480, height=480)
barplot(Bigram_DF[1:10,]$freq, las = 2, 
        names.arg = Bigram_DF[1:10,]$ngrams,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
dev.off()

png("Bigram_nostop.png", width=480, height=480)
wordcloud(words = Bigram_nostop_DF$ngrams, freq = Bigram_nostop_DF$freq, min.freq =10,
          max.words=50, random.order=FALSE, rot.per=0.25, 
          colors=brewer.pal(8, "Dark2"),scale=c(2,0.75))
dev.off()

png("Bigram_nostop_bar.png", width=480, height=480)
barplot(Bigram_nostop_DF[1:10,]$freq, las = 2, 
        names.arg = Bigram_nostop_DF[1:10,]$ngrams,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
dev.off()

setwd("../n_gram")
load("Trigram_DF.RData")
load("Trigram_nostop_DF.RData") 

setwd("../fig")
png("Trigram.png", width=480, height=480)
wordcloud(words = Trigram_DF$ngrams, freq = Trigram_DF$freq, min.freq = 10,
          max.words=50, random.order=FALSE, rot.per=0.25, 
          colors=brewer.pal(8, "Dark2"),scale=c(2,0.5))
dev.off()

png("Trigram_bar.png", width=480, height=480)
barplot(Trigram_DF[1:10,]$freq, las = 2, 
        names.arg = Trigram_DF[1:10,]$ngrams,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
dev.off()

png("Trigram_nostop.png", width=480, height=480)
wordcloud(words = Trigram_nostop_DF$ngrams, freq = Trigram_nostop_DF$freq, min.freq =1000,
          max.words=50, random.order=FALSE, rot.per=0.25, 
          colors=brewer.pal(8, "Dark2"),scale=c(2,0.5))
dev.off()

png("Trigram_nostop_bar.png", width=480, height=480)
barplot(Trigram_nostop_DF[1:10,]$freq, las = 2, 
        names.arg = Trigram_nostop_DF[1:10,]$ngrams,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
dev.off()
```

